from flask import Flask, request, jsonify
import joblib
import pandas as pd
import numpy as np
import warnings
import os
import requests 
import json 
import threading
import socket 

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore', category=UserWarning)

# --- ì„¤ì • ---
# ëª¨ë¸ ìì‚° ê²½ë¡œ
MODEL_PATH = 'random_forest_model.joblib'
SCALER_PATH = 'min_max_scaler.joblib'
LABEL_ENCODER_PATH = 'label_encoder.joblib'
FEATURE_NAMES_PATH = 'feature_names.joblib'
# API ì„œë²„ ì„¤ì •
HOST_IP = '0.0.0.0'
PORT = 5000          
# Ollama LLM API ì„¤ì • (í™˜ê²½ 2 ë¡œì»¬)
OLLAMA_URL = 'http://192.168.0.14:11434/api/generate' 
# Rule Command Client TCP ì„¤ì • (í™˜ê²½ 1ì˜ 10002 í¬íŠ¸)
RULE_COMMAND_CLIENT_HOST = '192.168.0.42'
RULE_COMMAND_CLIENT_PORT = 10002

app = Flask(__name__)

# ì „ì—­ ë³€ìˆ˜
model = None
scaler = None
le = None
feature_names = None
current_sid = 900000001 
sid_lock = threading.Lock() 

# --- 1. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ---

def load_assets():
    """ì €ì¥ëœ ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì¸ì½”ë”, íŠ¹ì§• ì´ë¦„ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global model, scaler, le, feature_names
    try:
        model = joblib.load(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        le = joblib.load(LABEL_ENCODER_PATH)
        feature_names = joblib.load(FEATURE_NAMES_PATH)
        print("âœ… ëª¨ë¸ ë° ìì‚° ë¡œë“œ ì™„ë£Œ. ì˜ˆì¸¡ ì¤€ë¹„ ì™„ë£Œ.")
        print(f"   ì˜ˆì¸¡ íŠ¹ì§• ê°œìˆ˜: {len(feature_names)}")
        return True
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: í•„ìš”í•œ íŒŒì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê²½ë¡œ í™•ì¸ í•„ìš”)")
        return False
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return False

# --- 2. SID ê´€ë¦¬ í•¨ìˆ˜ ---

def get_next_sid():
    """Rule SIDë¥¼ ì•ˆì „í•˜ê²Œ ì¦ê°€ì‹œí‚¤ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    global current_sid
    with sid_lock:
        sid = current_sid
        current_sid += 1
        return sid

# --- 3. LLM Rule ìƒì„± í•¨ìˆ˜ (ìµœì¢… ìµœì í™” í”„ë¡¬í”„íŠ¸ ì ìš©) ---

def generate_suricata_rule(attack_type, flow_features):
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ìµœì í™”ëœ Suricata Ruleì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    rule_action = "drop" 
    
    # Flow Feature ì¤‘ ìœ ì˜ë¯¸í•œ ê°’ë§Œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ (LLM Context Window ìµœì í™”)
    relevant_features = {k: v for k, v in flow_features.items() if v > 0.0 or k in ["Flow_Duration", "Total_Fwd_Packets"]}
    
    # ê³µê²© ìœ í˜•ì— ë”°ë¥¸ ì¶”ê°€ íŒíŠ¸ ì œê³µ
    additional_hints = ""
    if "Web Attack" in attack_type or "Injection" in attack_type or "Fuzzing" in attack_type:
        additional_hints = """
        Rule ì˜µì…˜: HTTP ê¸°ë°˜ ê³µê²©ì´ë¯€ë¡œ, 80/443 í¬íŠ¸ë¥¼ ëª…ì‹œí•˜ê³ , ê³µê²© ì‹œê·¸ë‹ˆì²˜ ë§¤ì¹­ì„ ìœ„í•´ 'content' ì˜µì…˜ê³¼ 
        ì•ˆì •ì ì¸ Flow ì¶”ì ì„ ìœ„í•´ 'flowbits:established' ì˜µì…˜ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. 
        """
    elif "PortScan" in attack_type:
        additional_hints = """
        Rule ì˜µì…˜: ëŒ€ëŸ‰ì˜ ì—°ê²° ì‹œë„ë¥¼ ì°¨ë‹¨í•´ì•¼ í•˜ë¯€ë¡œ, 'threshold' ì˜µì…˜(ì˜ˆ: type limit, track by_src, count 50, seconds 1)ì„ ì‚¬ìš©í•˜ì—¬ 
        ì„ê³„ì¹˜ ê¸°ë°˜ì˜ ë°©ì–´ ë¡œì§ì„ êµ¬í˜„í•˜ì„¸ìš”.
        """

    # ìµœì¢… ìµœì í™” í”„ë¡¬í”„íŠ¸
    prompt = f"""
    ë‹¹ì‹ ì€ ìµœê³  ìˆ˜ì¤€ì˜ Suricata ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ë„¤íŠ¸ì›Œí¬ì—ì„œ ë‹¤ìŒì˜ **ì•…ì„± ê³µê²©**ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤: **{attack_type}**
    ì£¼ìš” ë„¤íŠ¸ì›Œí¬ Flow Feature ì •ë³´ì…ë‹ˆë‹¤: {json.dumps(relevant_features, indent=2)}

    ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ **ì—„ê²©í•˜ê²Œ** ì¶©ì¡±í•˜ëŠ” **ë‹¨ í•˜ë‚˜ì˜ ìµœì í™”ëœ Suricata Rule ë¬¸ìì—´**ì„ ìƒì„±í•˜ì„¸ìš”:
    1. **ì•¡ì…˜**: ë°˜ë“œì‹œ **{rule_action}** ì•¡ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    2. **Rule í˜•ì‹**: ì˜¤ì§ Rule ë¬¸ìì—´ë§Œ ì¶œë ¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: `{rule_action} tcp $EXTERNAL_NET any -> $HOME_NET any (msg:"..."; sid:900000001; rev:1;)`)
    3. **ë³€ìˆ˜**: ë‚´ë¶€ë§ IPëŠ” **`192.168.0.23`** ëŒ€ì‹  **`$HOME_NET`** ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì™¸ë¶€ë§ì€ **`$EXTERNAL_NET`**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    4. **ë©”ì‹œì§€**: ë©”ì‹œì§€(msg)ëŠ” **`"AI_BLOCK:{attack_type}"`** í˜•íƒœë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
    5. **ìµœì í™” ì˜µì…˜**: ê³µê²© ìœ í˜•ê³¼ Feature ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ False Positiveë¥¼ ìµœì†Œí™”í•˜ì„¸ìš”. {additional_hints}
    6. Ruleì—ëŠ” sid:900000001; rev:1; ì˜µì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

    Rule:
    """

    data = {
        "model": "llama3",
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": 0.1, 
            "max_tokens": 512
        }
    }
    
    try:
        response = requests.post(OLLAMA_URL, json=data)
        response.raise_for_status()
        result = response.json()
        rule_text = result['response'].strip()
        
        # Rule ë¬¸ìì—´ ì •ë¦¬ ë¡œì§
        if rule_text.lower().startswith("rule:"):
            rule_text = rule_text[5:].strip()
            
        if not (rule_text.startswith(rule_action) and rule_text.endswith(")")):
            raise ValueError("LLMì´ ì˜¬ë°”ë¥¸ Suricata Rule í˜•ì‹ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        return rule_text

    except Exception as e:
        print(f"âŒ LLM Rule ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- 4. TCP ì „ì†¡ í•¨ìˆ˜ (Orchestrator í•µì‹¬) ---

def send_rule_command_to_client(command_json):
    """Rule Command Client (í™˜ê²½ 1: 10002 í¬íŠ¸)ë¡œ Rule ëª…ë ¹ì„ TCP ì†Œì¼“ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(3) # íƒ€ì„ì•„ì›ƒ 3ì´ˆ ì„¤ì •
    try:
        sock.connect((RULE_COMMAND_CLIENT_HOST, RULE_COMMAND_CLIENT_PORT))
        
        command_str = json.dumps(command_json)
        # ëª…ë ¹ ëì— ê°œí–‰ ë¬¸ìë¥¼ ì¶”ê°€í•˜ì—¬ Rule Command Clientê°€ ëª…ë ¹ì˜ ëì„ ì•Œ ìˆ˜ ìˆë„ë¡ í•¨
        sock.sendall((command_str + "\n").encode('utf-8')) 
        
        print(f"[AI-DR] â¡ï¸ Rule ëª…ë ¹ TCP ì „ì†¡ ì™„ë£Œ: {RULE_COMMAND_CLIENT_HOST}:{RULE_COMMAND_CLIENT_PORT}")
        return True
    except socket.timeout:
        print("âŒ Rule Command Client í†µì‹  ì‹¤íŒ¨: ì—°ê²° íƒ€ì„ì•„ì›ƒ.")
        return False
    except Exception as e:
        print(f"âŒ Rule Command Client í†µì‹  ì‹¤íŒ¨: {e}. í™˜ê²½ 1ì˜ 10002 í¬íŠ¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return False
    finally:
        sock.close()


# --- 5. ì˜ˆì¸¡ API ì—”ë“œí¬ì¸íŠ¸ (í†µí•© ì¤‘ì•™ ì œì–´ ë¡œì§) ---

@app.route('/predict/flow', methods=['POST'])
def predict_flow():
    """
    Flow Featureë¥¼ ì˜ˆì¸¡í•˜ê³ , ì•…ì„± íŠ¸ë˜í”½ì¸ ê²½ìš° LLM Rule ìƒì„± ë° í™˜ê²½ 1ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    if model is None:
        return jsonify({"error": "ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
    
    try:
        data = request.get_json(force=True)
        
        # 1. ë°ì´í„° ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
        input_data = {}
        for feature in feature_names:
            # í›ˆë ¨ëœ 77ê°œ feature ìˆœì„œ ë³´ì¥ ë° ëˆ„ë½ëœ featureëŠ” 0.0 ì²˜ë¦¬
            input_data[feature] = data.get(feature.strip().replace('_', ' '), 0.0)

        input_df = pd.DataFrame([input_data], columns=feature_names)
        input_df.replace([np.inf, -np.inf], np.nan, inplace=True)
        input_df.fillna(0, inplace=True) 

        X_scaled = scaler.transform(input_df)
        prediction_encoded = model.predict(X_scaled)[0]
        prediction_proba = model.predict_proba(X_scaled)[0]
        
        predicted_label = le.inverse_transform([prediction_encoded])[0]
        max_proba = np.max(prediction_proba)

        is_malicious = predicted_label != 'BENIGN' 

        response = {
            "status": "success",
            "predicted_attack_type": predicted_label,
            "confidence": f"{max_proba:.4f}",
            "is_malicious": is_malicious 
        }

        # 2. LLM Rule ìƒì„± ë° ì „ì†¡ ë¡œì§
        if is_malicious:
            print(f"\n[AI-DR] ğŸš¨ ì•…ì„± íƒì§€: {predicted_label}. LLM Rule ìƒì„± ìš”ì²­ ì¤‘...")
            
            raw_rule = generate_suricata_rule(predicted_label, input_data)
            
            if raw_rule:
                sid = get_next_sid()
                # LLMì´ ìƒì„±í•œ Rule ë¬¸ìì—´ì— ìµœì¢… SID ë° revë¥¼ ì‚½ì…/êµì²´
                final_rule_string = raw_rule.replace('sid:900000001;', f'sid:{sid};').replace('sid:900000001', f'sid:{sid}').strip()
                
                rule_command = {
                    "type": "ADD_RULE",
                    "rule": final_rule_string,
                    "sid": sid
                }
                
                # TCP ëª…ë ¹ ì „ì†¡
                if send_rule_command_to_client(rule_command):
                    print(f"[AI-DR] âœ… Rule ì ìš© ëª…ë ¹ ì „ì†¡ ì™„ë£Œ (SID: {sid}).")
                    response['rule_generation_status'] = "sent"
                    response['generated_rule'] = final_rule_string
                    response['rule_sid'] = sid
                else:
                    response['rule_generation_status'] = "tcp_send_failed"
            else:
                response['rule_generation_status'] = "llm_failed"
                print("[AI-DR] âŒ Rule ìƒì„± ì‹¤íŒ¨. Ollama ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        return jsonify(response)

    except Exception as e:
        print(f"ERROR: ì˜ˆì¸¡ ë˜ëŠ” Rule ìƒì„± ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({"error": f"ì˜ˆì¸¡ ë˜ëŠ” Rule ìƒì„± ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ: {e}"}), 500

# --- ì„œë²„ ì‹¤í–‰ ---
if __name__ == '__main__':
    if load_assets():
        print(f"\nğŸš€ ML & LLM API ì„œë²„ (Orchestrator í†µí•©) ì‹œì‘: http://{HOST_IP}:{PORT}")
        app.run(host=HOST_IP, port=PORT, debug=False)